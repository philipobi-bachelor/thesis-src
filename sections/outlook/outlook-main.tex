Given that AI technology will continue to evolve and demonstrates substantial potential across various domains, I believe it would be unwise not to engage with it, and explore potential applications for software optimization tasks.

While current AI models already prove helpful for software development, future models will likely improve further, making this a significant area of development for years to come.

A promising AI field for software optimization, in my opinion, is constraining AI models' outputs, similar to the approach employed by Google's AlphaDev system \cite{alphadev}.
AlphaDev, based on the AlphaZero architecture that achieved world-class performance in chess, shogi, and Go \cite{alphazero}, treated algorithm optimization as a strategic game.
The system operated with predefined moves, where each move represented adding a specific assembly instruction to the program.
Through this game-based approach, it was able to discover more efficient algorithms, such as a faster sorting algorithm for five-element sequences.

I find this approach to AI software optimization particularly elegant, as programming languages are inherently constrained by a predefined set of rules, making it possible to clearly determine whether an expression is valid or not.

This characteristic fundamentally differs from human language, which contains substantial nuance and ambiguity.
LLMs excel at natural language tasks precisely because their extensive parameters and training on large text corpora enable them to capture subtle patterns and generalize beyond training examples.
While this flexibility benefits natural language processing applications, it can be disadvantageous for rigid tasks such as programming, where precision and correctness are paramount.