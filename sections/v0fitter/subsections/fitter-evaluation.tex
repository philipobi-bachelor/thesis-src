{
\sloppy
\newcommand{\genSample}{v0Validation\allowbreak Generate\allowbreak Sample.py\xspace}
\newcommand{\genSim}{KShort\allowbreak GenSim\allowbreak NoBkg.py\xspace}
\newcommand{\genSimRoot}{KShort\allowbreak GenSim\allowbreak NoBkg.root\xspace}
\newcommand{\genPlots}{v0Validation\allowbreak Create\allowbreak Plots.py\xspace}

\subsubsection{Data Generation, Validation and Profiling Steering Scripts}
To validate the V0Fitter implementations, I used a slightly modified version of the basf2 steering script \genSample \cite{v0ValidationGenerateSample} (cf.\ Paragraph \hyperref[par:val-mod]{\emph{Modifications of the Validation Script}}).

\paragraph{Input Data}
\genSample requires a ROOT file as input, which is generated by the steering script \genSim \cite{KShortGenSimNoBkg}.
This script performs a Monte Carlo simulation of 1500 events without beam background overlay, providing clean signal data for validation purposes.

In each event, five KShort particles with uniformly distributed momentum magnitude and angular parameters are generated at the interaction point using the basf2 ParticleGun module \cite{ParticleGun-sphinx}.
Then, the particle interactions with the detector components, as well as the resulting detector response measurements are simulated.
This simulated measurement data is stored to \genSimRoot, which serves as input for \genSample.

\paragraph{Validation Script}
The script \genSample first performs track reconstruction on the generated data in \genSimRoot.
As a next step, it reconstructs V0 candidates using the V0Finder module and matches them to the true KShorts based on the simulation's particle identity information.
Finally, for each simulated KShort that decayed to pions, the script stores the simulated parameters (commonly referred to as the MC truth variables) and, if available, the parameters of a matched reconstructed V0 candidate to the output file V0ValidationHarvested.root.

\genPlots \cite{v0ValidationCreatePlots} generates a variety of validation plots from
the data in V0ValidationHarvested.root and stores them to V0Validation.root.
These include histograms of reconstructed physical quantities, e.g.\ the invariant mass of the KShort, and their residua%
\footnote{%
  Given a physical quantity $Q$ and a Monte Carlo V0 with a matched reconstructed V0, the residual is defined as $Q_\text{res.} \coloneq Q_\text{rec.} - Q_\text{MC}$
},
as well as plots of the reconstruction efficiency%
\footnote{Efficiency $\coloneq$ nReconstructed / nTotal}
as a function of variables such as V0 momentum and decay position (cf.\ Fig.\ \ref{fig:validation-plots}).

\paragraph{Comparison plots}
The script b2validation-plots \cite{b2validation-plots} allows to compare validation results from different code revisions.
Using the script's -r option, multiple revision directories \textemdash\ each containing validation outputs such as V0Validation.root \textemdash\ can be specified for comparison, with the first provided revision being treated as the reference.
b2validation-plots then overlays results from all selected revisions in comparison plots and calculates consistency metrics (e.g.\ $\chi^2$), color-coding the plots accordingly.

The script expects all revisions to be located in the same parent directory.
Since my directory structure did not meet this requirement, I implemented a helper script% 
\footnote{%
  \repoRef{validation/blob/main/validation-plots.py}{validation/validation-plots.py}
}
that symlinks selected validation results into a temporary directory with descriptive names (as they are used in the plot labels), runs b2validation-plots, collects the relevant outputs (PDF plots and consistency data) and then deletes the temporary directory.

\paragraph{Reproducibility and Determinism in Processing Results}
Although \genSim sets a random seed at the start of its execution, slight fluctuations in the generated data have been observed.
To improve reproducibility, I generated the \genSimRoot file once and used it as a consistent input for all subsequent validation runs.

Similarly, \genSample also initializes a random seed before processing. However, even with the unified use of the \genSimRoot file, the validation results did not appear to be fully deterministic.
For instance, the number of V0s reconstructed by the V0Finder fluctuated slightly when comparing the results of consecutive \genSample executions.\footnote{%
  Number of stored V0s from three consecutive \genSample executions, using the same machine and basf2 executable: 5896, 5895, 5894; cf.\ validation logs in \repoRef{validation/tree/main/validation-v0fitter-v0FitterMode1/e23d80d/runs}{validation/validation-v0fitter-v0FitterMode1/e23d80d/runs}
}

The source of this randomness is currently under investigation and is expected to be addressed in future versions of basf2. It is important to note that the observations described here were made using basf2 at the state of the release-09-00-00 commit and may not apply to earlier or later versions of the software.

\paragraph{Modifications of the Validation Script}\label{par:val-mod}
To enable validation of the different fitter implementations, I made slight modifications to the \genSample script.
First, as mentioned earlier, the input file path was updated to an absolute path pointing to the unified \genSimRoot file.
Additionally, to validate non-default V0Finder configurations \textemdash\ such as using the NewV0Fitter as the fitting backend instead of the default V0Fitter, or testing v0FitterModes 0 and 2 \textemdash\ the relevant parameters within \genSample were adjusted.
Aside from these changes, the script remained unaltered.

\paragraph{Custom Profiling Script}
To profile the three fitter versions, I used the script \genSample as a foundation and made significant modifications.
To begin with, the number of processed events was reduced from 1500 to 20. 
This adjustment was necessary since Callgrind profiling instrumentation drastically decreased execution speed%
\footnote{E.g.\ processing time for one event $\approx\qty{100}{\second}$ instead of $\approx\qty{660}{\milli\second}$ in optimized build
}, which made processing all events prohibitively time-consuming.
Additionally, I removed the matching of Monte Carlo V0s with their reconstructed counterparts and subsequent storage of their data to V0ValidationHarvested.root, as this process is unrelated to the actual fitter processing and thus only adds unnecessary runtime overhead during profiling.
To enable testing of each implementation, I set the useNewV0Fitter parameter in the script according to the specific fitter being profiled.
However, the v0FitterMode parameter was left at its default value of 1.
}

\subsubsection{Testing Methodology}
To make profiling and validation results reproducible across different code versions, I adopted the following approach:

\begin{minipage}[t]{0.58\textwidth}%
  \vspace{0pt}
  \begin{itemize}
    \item Each profiling or validation run was performed on its own branch in the basf2 repository, derived from the common release-09-00-00 commit (see Fig.\ \ref{fig:testing-branch-structure})
    \item Profiling and validation branches contain the unified steering scripts described in the previous paragraph, the only differences between individual branches are that relevant parameters were adjusted per branch: for example the unified validation script on branch validation-newv0fitter-v0fittermode2 configures the V0FinderModule with useNewV0Fitter: True and v0FitterMode: 2
    \item In order to test the effect of code modifications, the relevant development commit is first merged into the respective testing branches (cf.\ Fig.\ \ref{fig:testing-branch-structure}), then the branch containing the script to be run is checked out, the binaries recompiled if necessary, and finally basf2 is run using the branch's steering script.\par
    \emph{Note}: this procedure was only necessary for testing the V0Fitter refactor, as the existing implementations of V0Fitter and NewV0Fitter were not modified
    \item Profiling and validation results were saved in directories named after the shortened hashes of the commits they were generated with and are documented in dedicated repositories\footnotemark
  \end{itemize}
  This strategy ensures that test results are clearly linked to specific code versions, allowing for straightforward comparison across versions.

  In the following section, the short hashes in tables and images displaying validation and profiling results each refer to the commit which was used to generate the data.
\end{minipage}%
\hspace{15pt}%
\begin{minipage}[t]{0.4\textwidth}%
  \vspace{-15pt}
  \hspace{-10pt}%
  \includegraphics[width=1.1\linewidth]{static/project/artifacts/testing-branch-structure.pdf}
  \vspace{-30pt}
  \captionof{figure}{
    Testing branch structure, illustrated for the profiling branches.
    The validation branches have similar structure, however there is an additional dimension of subdivision based on the v0FitterMode value
    }
  \label{fig:testing-branch-structure}
\end{minipage}%
\footnotetext{
  \repoRef{profiling}{profiling} and \repoRef{validation}{validation}
}

\subsubsection{Comprehensive Testing Results for the V0Fitter Implementations}
Throughout the refactoring process, I continuously compared the refactored V0Fitter's performance with that of the two original implementations, which enabled me to assess how individual modifications affected computational efficiency and reconstruction quality.
This iterative evaluation shaped my coding approach, prompting me to revise or discard some optimization efforts before arriving at the current implementation described in the previous section.

Comparing profiling results of the refactored version and the prior implementations reveals a significant decrease in total executed instructions, primarily due to elimination of genfit::Track copy constructor invocations and fewer calls to TrackFitter::fit compared to the original V0Fitter implementation (Fig.\ \ref{fig:profiling-refactor}, Tab.\ \ref{tab:profiling-all}).

The performed optimizations also result in noticeable speedups when running the full validation scripts on optimized builds (Tab.\ \ref{tab:validation}).

\begin{table}[h]
  \begin{center}
    \import{tables/v0fitter/}{profiling-all.tex}%
    \makebox[\linewidth]{\tab}
  \end{center}
  \caption{Comprehensive Profiling Metrics for the V0Fitter Implementations}
  \label{tab:profiling-all}
\end{table}

\begin{figure}[h]
  \vspace{-20pt}
  \centering
  \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{static/profiling/profiling-v0fitter/050e500/pruned-graph.pdf}
  \end{subfigure}

  \vspace{-25pt}

  {\color{gray}\rule{\linewidth}{0.5pt}}

  \vspace{-15pt}

  \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=0.7\linewidth]{static/profiling/profiling-v0fitter-refactor/a4aaf96/pruned-graph.pdf}
  \end{subfigure}

  \vspace{-20pt}

  \caption{
    Call graph segments starting from the fitAndStore class entry point, annotated with profiling data. \emph{Top:} V0Fitter, \emph{Bottom:} Refactored V0Fitter
  }
  \label{fig:profiling-refactor}
\end{figure}


Judging by the validation results, i.e.\ comparing the statistics on the stored V0s and generating plots from the collected ROOT data, the refactored V0Fitter maintains a reconstruction efficiency similar to that of the previous versions.
The number of stored V0s for a given fitter configuration is relatively similar across the different implementations (Tab.\ \ref{tab:validation}), and the plots, using the original V0Fitter data as reference, match closely (Fig.\ \ref{fig:validation-plots}).

\begin{table}[h]
  \centering
  \import{tables/v0fitter/}{validation.tex}
  \caption{Comprehensive Validation Metrics for the V0Fitter Implementations}\label{tab:validation}
\end{table}

\begin{figure}[h]
  \centering
  \begin{subfigure}{.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{static/validation/plots-v0FitterMode1/tracking/V0Validation_EfficiencyvsR.pdf}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    \centering
    \includegraphics[width=\linewidth]{static/validation/plots-v0FitterMode1/tracking/V0Validation_KShortInvariantMass.pdf}
  \end{subfigure}
  \caption{Plots comparing the validation outputs of the V0Fitter (labelled \emph{reference}, commit 050e500), NewV0Fitter and refactored V0Fitter}
  \label{fig:validation-plots}
\end{figure}

The tolerance parameter introduced for the invariant mass cuts in the refactored version allows to control the number of track pairs considered for inner hit removal after the initial vertex fit.
By adjusting this value, it is possible to balance between the highest computational efficiency (tolerance $= 0$) and the greatest reconstruction sensitivity (tolerance $\gg 1$).
Since the inner hit removal has the highest computational overhead in the V0 finding process per call (\ref{tab:profiling-all}), it appears worthwhile to investigate, whether more advanced heuristics can be developed to better judge how promising a V0 candidate is after the initial vertex fit.

